# Big Data ETL Pipeline for User Behavior and User Searching Behavior Analytics with PySpark

This project demonstrates an end-to-end ETL (Extract, Transform, Load) pipeline built with **Apache Spark (PySpark)** to process and analyze large-scale TV content viewing and searching content data. The final dataset is loaded into a **BigQuery** database and visualized using **Power BI**.

---

## ğŸ“Œ Project Overview

- âœ… Built with **PySpark** to handle **large-scale JSON logs** and **large-scale Parquet logs** efficiently.
- 1. User behavior:
  + âœ… Categorized content based on `AppName` into user-friendly types (e.g., TV, Movies, Entertainment, Sports).
  + âœ… Calculated key **user behavior metrics**:
    - **Most watched Content Type**
    - **Taste Preferences**
    - **Watching Total hours  by channel**
- 2. User searching behavior:
  + âœ… Categorized searching keyword based on `Keyword` into detail category movies/shows (e.g., TV, Movies, Entertainment, Sports).
  + âœ… Calculated key **user searching behavior metrics**:
    - **Most watched Content Type**
    - **Taste Preferences**
    - **Watching Total hours  by channel**
- âœ… Stored results in **BigQuery** for downstream analytics or BI integration.
- âœ… Visualized results in an interactive **Power BI dashboard**.

---

## ğŸ› ï¸ Tech Stack

| Component      | Description                                       |
|----------------|---------------------------------------------------|
| PySpark        | Distributed processing of JSON data               |
| Gemini API     | Categorize searching keyword                      |
| BigQuery       | Target data warehouse for storing aggregated data |
| Power BI       | Visualization of final behavioral metrics         |
| JSON & Parquet | Raw input data format                             |

---
## ğŸ—‚ï¸ Project Structure
customer_360-etl_pipeline/
â”‚

â”œâ”€â”€ explore_log_content.py # Main ETL script for User Content Consumption Behavior

â”œâ”€â”€ explore_log_search.py # Main ETL script for User Searching Behavior

â”œâ”€â”€ data(sample data )/

â”‚   â”œâ”€â”€ log_content/

â”‚   â”‚   â”œâ”€â”€ YYYYMMDD.json # VÃ­ dá»¥ tá»‡p log ná»™i dung


â”‚   â”œâ”€â”€ log_search/

â”‚   â”‚   â”œâ”€â”€ YYYYMMDD.parquet # VÃ­ dá»¥ tá»‡p log tÃ¬m kiáº¿m

â”œâ”€â”€ Power_BI

â”‚   â”œâ”€â”€ Content_Consumption_and_Searching_Insights.pbix # Power BI dashboard file 

â”œâ”€â”€ google_cloud_platform_account_service_keys.json

â”œâ”€â”€ .env 

â”‚
â”œâ”€â”€ README.md # Project documentation

## ğŸ”„ ETL Flow for User Content Consumption

1. **Extract**
   - Load JSON files daily from a folder.
   - Flatten nested fields and normalize the schema.

2. **Transform**
   - Categorize `AppName` into content types.
   - Pivot data by user `Contract` and `Content Type`.
   - Compute behavior metrics:
     - `Mostwatch`: Dominant content type watched
     - `Taste`: All genres the user interacts with
     

3. **Load**
   - Append final transformed data into BigQuery table: `viewing_time`.

ğŸ“ˆ Sample Output
## ğŸ“ˆ Sample Output Schema

| Column         | Type       | Description                                 |
|----------------|------------|---------------------------------------------|
| contract       | string     | Unique user identifier                      |
| giai_tri       | bigint     | Total duration of Entertainment content     |
| phim_truyen    | bigint     | Total duration of Movie content             |
| the_thao       | bigint     | Total duration of Sports content            |
| thieu_nhi      | bigint     | Total duration of Children content          |
| truyen_hinh    | bigint     | Total duration of TV content                |
| most_watch     | string     | Most-watched content type                   |
| taste          | string     | Genres the user interacted with             |
## ğŸ’¡ Sample Output

<img src="./img/sample_user_content_consumption_data.png">

## ğŸ”„ ETL Flow for User Searching Behavior

### ğŸ” Step 1: Read Large-Scale Parquet Logs
- Load daily search logs (June & July)
- Combine and cache for performance

### ğŸ“Š Step 2: Identify Most Searched Keyword per User
- Group by `user_id` and `keyword`
- Use `row_number()` with Spark Window function to select the top search per user

### ğŸ” Step 3: Compare Behavior Across Months
- Join June and July datasets by `user_id`
- Identify users whose **search category changed**

### ğŸ§© Step 4: Enrich Keyword Categories with Gemini API
- Call Gemini API to categorize keyword base on default specific categories
- Add dataframe `keyword_category`

### ğŸ§© Step 5: Enrich with Keyword Categories
- Join with `keyword_category` dataframe created above step 
- Add `category_t6`, `category_t7`, and `category_change`

### ğŸ’¾ Step 6: Store Results in BigQuery
- Final schema: `trending`

## ğŸ§¾ Output Table Schema (MySQL)

| Column           | Type   | Description                                |
|------------------|--------|--------------------------------------------|
| user_id          | text   | Unique user identifier                     |
| most_search_t6   | text   | Most searched keyword in June              |
| category_t6      | text   | Corresponding category of the June keyword |
| most_search_t7   | text   | Most searched keyword in July              |
| category_t7      | text   | Corresponding category of the July keyword |
| category_change  | text   | 'unchanged' or "changed" if changed        |
## ğŸ’¡ Sample Output
<img src="./img/favourite_type_each_user.png">


## ğŸ“Š Power BI Dashboard

# User content consumption
![Dashboard](/img/user_behavior.png)

---
# User searching behavior

